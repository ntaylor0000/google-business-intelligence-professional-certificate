## The Path to Insights: Data Models and Pipelines

- [2 - Foundations of Business Intelligence](https://www.coursera.org/learn/the-path-to-insights-data-models-and-pipelines)

This is the second of three courses in the Google Business Intelligence Certificate. In this course, you'll explore data modeling and how databases are designed. Then you’ll learn about extract, transform, load (ETL) processes that extract data from source systems, transform it into formats that enable analysis, and drive business processes and goals.

Google employees who currently work in BI will guide you through this course by providing hands-on activities that simulate job tasks, sharing examples from their day-to-day work, and helping you build business intelligence skills to prepare for a career in the field. 

Learners who complete the three courses in this certificate program will have the skills needed to apply for business intelligence jobs. This certificate program assumes prior knowledge of foundational analytical principles, skills, and tools covered in the Google Data Analytics Certificate.   

By the end of this course, learners will:
- Determine which data models are appropriate for different business requirements
- Describe the difference between creating and interacting with a data model
- Create data models to address different types of questions
- Explain the parts of the extract, transform, load (ETL) process and tools used in ETL
- Understand extraction processes and tools for different data storage systems
- Design an ETL process that meets organizational and stakeholder needs
- Design data pipelines to automate BI processes

<details>
<summary>Module 1</summary>
<h6 align="left">
  
**Data models and pipelines**

You’ll start this course by exploring data modeling, common schemas, and database elements. You’ll consider how business needs determine the kinds of database systems that BI professionals implement. Then, you’ll discover pipelines and ETL processes, which are tools that move data and ensure that it’s accessible and useful.
  
**Learning Objectives**
- Identify and define key database models and schemas.
- Assess which database design pattern and schema is appropriate for different data.
- Discuss data model alternatives that would be optimal, performant, and adherent to the reporting requirements looking into current data size and growth.
- Define ETL and explain what it means.
- Identify key information from stakeholders necessary to create a data pipeline.
- Describe different types of pipelines.
- Describe the key stages of a data pipeline.
- Understand what a data pipeline is, its objectives, and how it works.

**Lessons**
- Get started with data modeling, schemas, and databases
- Choose the right database
- How data moves
- Data-processing with Dataflow
- Organize data in BigQuery
- Review: Data models and pipelines
- [Optional] Review Google Data Analytics Certificate content
  
</h6>
</details>
<details>
<summary>Module 2</summary>
  
**Dynamic database design**

You’ll learn more about database systems, including data marts, data lakes, data warehouses, and ETL processes. You’ll also investigate the five factors of database performance: workload, throughput, resources, optimization, and contention. Finally, you’ll consider how to design efficient queries that get the most from a system.

**Learning Objectives**
- Discover strategies to create an ETL process that works to meet organizational and stakeholder needs and maintain an ETL process efficiently.
- Understand what the different data storage and extraction processes and tools may include (Extract/L: Stitch/Segment/Fivetran, Transform: DBT/Airflow/Looker).
- Explain how to optimize when building new tables.
- Identify and describe where new tables can fit in the pipeline.
- Recognize the different aspects of databases, including OLAP and OLTP, columnar and relational, distributed and single-homed databases.
- Understand the importance of database performance and optimization.
- Describe the different five factors of database performance: workload, throughput, resources, optimization, and contention.
- Perform pipeline debugging using queries.

**Lessons**
- Database performance
- Review: Dynamic database design

</h6>
</details>
<details>
<summary>Module 3</summary>
                   
**Optimize ETL processes**

You’ll learn about optimization techniques including ETL quality testing, data schema validation, business rule verification, and general performance testing. You’ll also explore data integrity and learn how built-in quality checks defend against potential problems. Finally, you’ll focus on verifying business rules and general performance testing to make sure pipelines meet the intended business need.

**Learning Objectives**
- Discover strategies to create an ETL process that works to meet organizational and stakeholder needs and how to maintain an ETL process efficiently.
- Introduce tools used in ETL
- Understand the primary goals of ETL quality testing.
- Understand the primary goals of data schema validation.
- Develop ETL quality testing and data schema validation best practices.
- Identify and implement appropriate test scenarios and checkpoints for QA on data pipelines.
- Explain different methods for QA data in the pipeline.
- Create performance testing scenarios and measure performance throughout the pipeline.
- Verify business rules.
- Perform general performance testing.

**Lessons**
- COptimizing pipelines and ETL processes
- Data schema validation
- Business rules and performance testing
- Review: Optimize ETL processes
- [Optional] Review Google Data Analytics content

</h6>
</details>
<details>
<summary>Module 4</summary>

**Course 2 end-of-course project**

You’ll complete an end-of-course project by creating a pipeline process to deliver data to a target table and developing reports based on project needs. You’ll also ensure that the pipeline is performing correctly and that there are built-in defenses against data quality issues.

**Learning Objectives**
- Identify business needs to determine a design for your portfolio project’s data pipeline.
- Analyze how database systems are designed, how to build BI tools such as pipelines and ETL systems, and how to optimize them to maximize performance to determine the most optimal data pipeline process.
- Develop a data pipeline to deliver necessary data to a target table.

**Lessons**
- Apply your skills to a workplace scenario
- Cyclistic scenario
- Google Fiber scenario
- End-of-course project wrap-up
- Course review: The Path to Insights: Data Models and Pipelines

</h6>
</details>
